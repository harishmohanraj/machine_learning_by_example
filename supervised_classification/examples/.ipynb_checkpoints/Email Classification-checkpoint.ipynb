{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The purpose of this program is to classify the emails \n",
    "based on a training dataset and predict the upcoming emails\n",
    "into their corresponding categories\n",
    "\"\"\"\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# importing email dataset from the csv\n",
    "dataset = pd.read_csv('test.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the special characters by empty strings\n",
    "dataset['Subject'] = dataset['Subject'].map(lambda x: x.replace('\\xe6', ''))\n",
    "dataset['Subject'] = dataset['Subject'].map(lambda x: x.replace('\\r', ''))\n",
    "dataset['Message'] = dataset['Message'].map(lambda x: x.replace('\\xe6', ''))\n",
    "dataset['Message'] = dataset['Message'].map(lambda x: x.replace('\\r', ''))\n",
    "\n",
    "# Replacing the string labels into 0,1,2. Please refer to the below table for mapping\n",
    "# 'Credit and account statements' => 0\n",
    "# 'Direct debit and Bacs' => 1\n",
    "# 'Account closures' => 2\n",
    "dataset['Class'] = dataset['Class'].map(lambda x: x.replace('Credit and account statements', '0'))\n",
    "dataset['Class'] = dataset['Class'].map(lambda x: x.replace('Direct debit and Bacs', '1'))\n",
    "dataset['Class'] = dataset['Class'].map(lambda x: x.replace('Account closures', '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the features and labels from the cleaned dataset\n",
    "# Converting the 2d features dataset into 1d featrures dataset\n",
    "features = dataset[['Subject','Message']].values;\n",
    "one_d_features = []\n",
    "for i in features:\n",
    "    one_d_features.append(\"\".join(i).split(\"/n\"))\n",
    "one_d_features = [j for i in one_d_features for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features lenght:  42\n",
      "Training labels lenght:  42\n",
      "Testing features lenght:  19\n",
      "Testing labels lenght:  19\n"
     ]
    }
   ],
   "source": [
    "# Implementing cross validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(one_d_features, labels, \n",
    "                                                                            test_size=0.30, random_state=42)\n",
    "print(\"Training features lenght: \",len(features_train))\n",
    "print(\"Training labels lenght: \",len(labels_train))\n",
    "print(\"Testing features lenght: \",len(features_test))\n",
    "print(\"Testing labels lenght: \",len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting the 2d labels dataset into 1d labels dataset\n",
    "labels_train = np.array(sum(labels_train, []))\n",
    "labels_test = np.array(sum(labels_test, []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '2' '0' '2' '2' '1' '2' '0' '2' '1' '0' '1' '1' '1' '1' '2' '0' '1'\n",
      " '2' '2' '1' '1' '2' '0' '1' '0' '2' '1' '1' '1' '2' '0' '1' '0' '2' '1'\n",
      " '0' '2' '0' '1' '2' '1']\n"
     ]
    }
   ],
   "source": [
    "# Bag of words method\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#clf = MultinomialNB().fit(X_train_tfidf, labels_train)\n",
    "#from sklearn import svm\n",
    "#clf = svm.SVC().fit(X_train_tfidf, labels_train)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier().fit(X_train_tfidf, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(features_train, labels_train)\"\"\"\n",
    "\n",
    "\"\"\"from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', svm.SVC(kernel=\"linear\"))])\n",
    "text_clf = text_clf.fit(features_train, labels_train)\"\"\"\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', RandomForestClassifier())])\n",
    "text_clf = text_clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0.631578947368\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predicted = text_clf.predict(features_test)\n",
    "print( \"Mean: \",np.mean(predicted == labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  63.1578947368\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(predicted, labels_test)\n",
    "print (\"Accuracy: \",acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python with Pixiedust (Spark 2.2)",
   "language": "python",
   "name": "pythonwithpixiedustspark22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
